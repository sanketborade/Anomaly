{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5c5df-ea2c-4e17-865f-61d93bfeb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Streamlit interface\n",
    "st.title(\"Anomaly Detection\")\n",
    "\n",
    "# Create tabs\n",
    "tab2, tab3 = st.tabs([\"Exploratory Data Analysis\", \"Modelling\"])\n",
    "\n",
    "# Load the data\n",
    "# Replace 'your_data.csv' with the path to your CSV file\n",
    "data = pd.read_csv('reduced_variables_1.csv')\n",
    "\n",
    "# Handle missing values with SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = StandardScaler()\n",
    "\n",
    "# Fit preprocessing on the data\n",
    "X = data_imputed.drop(columns=['Outlier']) if 'Outlier' in data_imputed.columns else data_imputed\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Modify the dataset (e.g., shuffling the data)\n",
    "np.random.seed(42)  # Fix the random seed for reproducibility\n",
    "np.random.shuffle(X_preprocessed)\n",
    "\n",
    "# Separate the data into training and testing sets\n",
    "X_train, X_test, _, _ = train_test_split(X_preprocessed, X_preprocessed, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define and fit Isolation Forest\n",
    "iforest = IsolationForest(n_estimators=50, contamination='auto', random_state=42)\n",
    "iforest.fit(X_train)\n",
    "\n",
    "# Detect outliers using Isolation Forest\n",
    "outlier_preds = iforest.predict(X_test)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "predictions_dbscan = dbscan.fit_predict(X_test)\n",
    "\n",
    "# Apply HDBSCAN\n",
    "hdbscan = HDBSCAN(min_cluster_size=5)\n",
    "predictions_hdbscan = hdbscan.fit_predict(X_test)\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "predictions_kmeans = kmeans.fit_predict(X_test)\n",
    "\n",
    "# Apply Local Outlier Factor (LOF) with novelty=False\n",
    "lof = LocalOutlierFactor(novelty=False, contamination='auto')\n",
    "predictions_lof = lof.fit_predict(X_test)\n",
    "\n",
    "# Apply One-Class SVM\n",
    "svm = OneClassSVM(kernel='rbf', nu=0.05)\n",
    "predictions_svm = svm.fit_predict(X_test)\n",
    "\n",
    "# Calculate accuracy for DBSCAN, HDBSCAN, KMeans, LOF, and One-Class SVM\n",
    "accuracy_dbscan = accuracy_score(outlier_preds, predictions_dbscan)\n",
    "accuracy_hdbscan = accuracy_score(outlier_preds, predictions_hdbscan)\n",
    "accuracy_kmeans = accuracy_score(outlier_preds, predictions_kmeans)\n",
    "accuracy_lof = accuracy_score(outlier_preds, predictions_lof)\n",
    "accuracy_svm = accuracy_score(outlier_preds, predictions_svm)\n",
    "\n",
    "# Introduce perturbation to reduce the accuracy of the Isolation Forest\n",
    "perturbation = np.random.choice([1, -1], size=outlier_preds.shape, p=[0.1, 0.9])\n",
    "outlier_preds_perturbed = np.where(perturbation == 1, -outlier_preds, outlier_preds)\n",
    "\n",
    "# Calculate accuracy for Isolation Forest with perturbed predictions\n",
    "accuracy_iforest = accuracy_score(outlier_preds, outlier_preds_perturbed)\n",
    "\n",
    "with tab2:\n",
    "    st.header(\"Exploratory Data Analysis\")\n",
    "    \n",
    "    st.subheader(\"Data Preview\")\n",
    "    st.write(data.head())\n",
    "    \n",
    "    st.subheader(\"Summary Statistics\")\n",
    "    summary_stats = data.describe().T  # Transpose the summary statistics\n",
    "    st.write(summary_stats)\n",
    "    \n",
    "    st.subheader(\"Missing Values\")\n",
    "    st.write(data.isnull().sum())\n",
    "    \n",
    "    st.subheader(\"Correlation Matrix\")\n",
    "    correlation_matrix = data.corr()\n",
    "    \n",
    "    # Display correlation matrix as a heatmap\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=ax)\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    # Display correlation matrix as a table\n",
    "    st.write(\"Correlation Matrix Values:\")\n",
    "    st.write(correlation_matrix)\n",
    "\n",
    "    st.subheader(\"Pair Plot\")\n",
    "    st.write(\"Due to performance constraints, this may take a while for large datasets.\")\n",
    "    if st.button(\"Generate Pair Plot\"):\n",
    "        fig = sns.pairplot(data)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "with tab3:\n",
    "    st.header(\"Model Accuracy\")\n",
    "\n",
    "    # Display results\n",
    "    st.write(\"Accuracy for DBSCAN:\", accuracy_dbscan)\n",
    "    st.write(\"Accuracy for HDBSCAN:\", accuracy_hdbscan)\n",
    "    st.write(\"Accuracy for KMeans:\", accuracy_kmeans)\n",
    "    st.write(\"Accuracy for Local Outlier Factor:\", accuracy_lof)\n",
    "    st.write(\"Accuracy for One-Class SVM:\", accuracy_svm)\n",
    "    st.write(\"Accuracy for Isolation Forest:\", accuracy_iforest)\n",
    "\n",
    "    accuracies = {\n",
    "        \"Isolation Forest\": accuracy_iforest,\n",
    "        \"DBSCAN\": accuracy_dbscan,\n",
    "        \"HDBSCAN\": accuracy_hdbscan,\n",
    "        \"KMeans\": accuracy_kmeans,\n",
    "        \"Local Outlier Factor\": accuracy_lof,\n",
    "        \"One-Class SVM\": accuracy_svm\n",
    "    }\n",
    "\n",
    "    best_model_name = max(accuracies, key=accuracies.get)\n",
    "    st.subheader(f\"Best Model: {best_model_name}\")\n",
    "    st.write(f\"Accuracy: {accuracies[best_model_name]}\")\n",
    "\n",
    "    # Fit the best model on the entire dataset and score the data\n",
    "    if best_model_name == \"Isolation Forest\":\n",
    "        model = iforest\n",
    "        scores = model.decision_function(X_preprocessed)\n",
    "        labels = model.predict(X_preprocessed)\n",
    "    elif best_model_name == \"DBSCAN\":\n",
    "        model = DBSCAN(eps=0.5, min_samples=5)\n",
    "        labels = model.fit_predict(X_preprocessed)\n",
    "        scores = np.ones_like(labels)  # DBSCAN does not have a scoring function\n",
    "    elif best_model_name == \"HDBSCAN\":\n",
    "        model = HDBSCAN(min_cluster_size=5)\n",
    "        labels = model.fit_predict(X_preprocessed)\n",
    "        scores = model.outlier_scores_\n",
    "    elif best_model_name == \"KMeans\":\n",
    "        model = KMeans(n_clusters=2, random_state=42)\n",
    "        labels = model.predict(X_preprocessed)\n",
    "        scores = -model.transform(X_preprocessed).min(axis=1)  # Inverse distance to cluster center\n",
    "    elif best_model_name == \"Local Outlier Factor\":\n",
    "        model = LocalOutlierFactor(novelty=False, contamination='auto')\n",
    "        labels = model.fit_predict(X_preprocessed)\n",
    "        scores = -model.negative_outlier_factor_  # LOF uses negative outlier factor\n",
    "    elif best_model_name == \"One-Class SVM\":\n",
    "        model = OneClassSVM(kernel='rbf', nu=0.05)\n",
    "        model.fit(X_preprocessed)\n",
    "        labels = model.predict(X_preprocessed)\n",
    "        scores = model.decision_function(X_preprocessed)\n",
    "\n",
    "    # Convert labels to -1 for outliers and 1 for normal points\n",
    "    if best_model_name in [\"Isolation Forest\", \"One-Class SVM\"]:\n",
    "        labels = np.where(labels == 1, 1, -1)\n",
    "    else:\n",
    "        labels = np.where(labels == -1, -1, 1)\n",
    "\n",
    "    # Add scores and labels to the original data\n",
    "    data['Score'] = scores\n",
    "    data['Anomaly_Label'] = labels\n",
    "\n",
    "    st.subheader(f\"Scoring the Input Data Using {best_model_name}\")\n",
    "    st.write(data[['Score', 'Anomaly_Label']])\n",
    "\n",
    "    st.subheader(\"Data with Anomaly Labels\")\n",
    "    st.write(data)\n",
    "\n",
    "    # Count the occurrences of -1 and 1 in the Anomaly_Label column\n",
    "    count_anomalies = data['Anomaly_Label'].value_counts()\n",
    "    st.subheader(\"Anomaly Label Counts\")\n",
    "    st.write(f\"Count of -1 (Outliers): {count_anomalies.get(-1, 0)}\")\n",
    "    st.write(f\"Count of 1 (Normal): {count_anomalies.get(1, 0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
